% !TeX root = ../main.tex

\chapter{绪论}
\section{研究背景}
很多现实世界的系统可以被建模成多智能体系统(MAS), 比如自动交通队伍~\cite{cao2012overview}, 智能仓库系统~\cite{nowe2012game}, 感受器网络~\cite{zhang2011coordinated}等等。而协同多智能体强化学习(MARL)提供了一个很有前景的去处理这些问题的方式，因为MARL能让智能体处理不确定的环境，以及能够逐步适应环境。近些年，协同多智能体强化学习取得了很大的进步，有很多基于深度学习的算法被提出来~\cite{foerster2018counterfactual, sunehag2018value, rashid2018qmix, son2019qtran, vinyals2019grandmaster, wang2020learning, baker2020emergent}。

\section{国内外研究现状}
为了获得可拓展性，现在的多智能体深度强化学习框架往往采用一个很简化的模式，即所有的智能体都共享以及学习一个分离的值网络或者策略网路，但是这样简单的共享往往是不够的，特别是对于一些复杂的多智能体任务。举例来说，在Adam Smith的大头针工厂里，工人必须要完成18个完全不同的子任务才能制作成一个大头针~\cite{smith1937wealth}。在这个情形下，只用一个共享的网络往往是很重的负担，因为它要同时去学很多完全不同的策略，还要去表达不同的技能。在另一方面，每个智能体都单独有一个网络也是没必要的，因为有一些工人会有相同的任务，并且各自独立也会增加计算复杂度。

所以问题就是如何能充分发挥智能体的的专业化以及动态的信息、参数共享来提高性能。

一个很自然的概念会出现，就是角色。角色是行为模式的抽象，一个角色常常专注于某些任务。拥有相似角色的智能体会有相似的行为，因而能够共享它们的经验以来提高性能。

事实上，角色理论已经在经济学、社会学和组织学中被广泛地研究了。科研人员以前也将角色概念引入到多智能体强化学习中~\cite{becht1999rope, stone1999task, depke2001roles, ferber2003agents, odell2004metamodel, bonjean2014adelfe, Lhaksmana2018role}。在这些基于角色的框架中，往往是通过任务分解、将角色与子任务联系起来来完成智能体设计的~\cite{zhu2008role}. 然而，这些工作需要利用领域知识来完成任务分解以及需要预定义每个角色的责任，这些限制了基于角色的多智能体系统应用到动态的、变化的、未知的环境中。

\section{主要工作与创新点}
为了同时利用角色理论以及深度学习方法，本项目提出了一个基于角色的多智能体强化学习框架(Role-Oriented Multi-Agent Reinforcement Learning), 并命名为ROMA.

本框架隐式地将角色概念引入到多智能体强化学习，让角色充当媒介，使得拥有相似角色的智能体能够共享它们的训练学习。为了达到这一点，本框架让拥有相似角色的智能体同时有相似的策略和子任务。

为了将角色和智能体的策略联系起来，本框架将智能体的策略决定于它自己的角色，而角色又来自又它自己的观测决定的角色空间的采样。为了将角色和子任务联系起来，本框架提出了两个损失函数，使得角色能够通过它的行为被识别出来，以及使得角色专注于某些子任务。

本文会展示，通过优化损失函数的变分估计，能得到结构良好的角色表征，以及优异的性能。

本项目的实验均是在星际争霸2微操作环境~\cite{vinyals2017starcraft, samvelyan2019starcraft}中进行的。实验结果表明，本算法通过让拥有相似角色的智能体共享策略、学习，极大地推进了多智能体强化学习的科研进程。对同质和异质环境下的角色表征的可视化也说明了学到的角色能够动态地适应环境，以及拥有相似子任务的智能体有相似的角色。除此之外，对角色演化和涌现的分析，也说明了角色驱动的子任务专业化和集体性能的提高有很大的关联。这些结果也提供了一个理解和促进智能体的某些涌现和写作的新视角。

\section{论文组织结构}
本文按如下展开，第二章讲背景知识，第三章详细地阐释了本框架，第四章分析实验结果，第五章做简要的总结和展望。